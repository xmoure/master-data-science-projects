Vw <- calcWithinGroupsVariance(variablei, groupvariable)
Vb <- calcBetweenGroupsVariance(variablei, groupvariable)
sep <- Vb/Vw
print(paste("variable",variablename,"Vw=",Vw,"Vb=",Vb,"separation=",sep))
}
}
calcAllocationRuleAccuracy <- function(ldavalue, groupvariable, cutoffpoints)
{
# find out how many values the group variable can take
groupvariable2 <- as.factor(groupvariable[[1]])
levels <- levels(groupvariable2)
numlevels <- length(levels)
# calculate the number of true positives and false negatives for each group
numlevels <- length(levels)
for (i in 1:numlevels)
{
leveli <- levels[i]
levelidata <- ldavalue[groupvariable==leveli]
# see how many of the samples from this group are classified in each group
for (j in 1:numlevels)
{
levelj <- levels[j]
if (j == 1)
{
cutoff1 <- cutoffpoints[1]
cutoff2 <- "NA"
results <- summary(levelidata <= cutoff1)
}
else if (j == numlevels)
{
cutoff1 <- cutoffpoints[(numlevels-1)]
cutoff2 <- "NA"
results <- summary(levelidata > cutoff1)
}
else
{
cutoff1 <- cutoffpoints[(j-1)]
cutoff2 <- cutoffpoints[(j)]
results <- summary(levelidata > cutoff1 & levelidata <= cutoff2)
}
trues <- results["TRUE"]
trues <- trues[[1]]
print(paste("Number of samples of group",leveli,"classified as group",levelj," : ",
trues,"(cutoffs:",cutoff1,",",cutoff2,")"))
}
}
}
printMeanAndSdByGroup <- function(variables,groupvariable)
{
# find the names of the variables
variablenames <- c(names(groupvariable),names(as.data.frame(variables)))
# within each group, find the mean of each variable
groupvariable <- groupvariable[,1] # ensures groupvariable is not a list
means <- aggregate(as.matrix(variables) ~ groupvariable, FUN = mean)
names(means) <- variablenames
print(paste("Means:"))
print(means)
# within each group, find the standard deviation of each variable:
sds <- aggregate(as.matrix(variables) ~ groupvariable, FUN = sd)
names(sds) <- variablenames
print(paste("Standard deviations:"))
print(sds)
# within each group, find the number of samples:
samplesizes <- aggregate(as.matrix(variables) ~ groupvariable, FUN = length)
names(samplesizes) <- variablenames
print(paste("Sample sizes:"))
print(samplesizes)
}
#if features are normalized, then results are more understandable
training_standardise <- groupStandardise(training[,c(1:6)], training[7])
training_standardise$f.rating <- training$f.rating
test_standardise <- groupStandardise(test[,c(1:6)], test[7])
test_standardise$f.rating <- test$f.rating
summary(training_standardise)
summary(test_standardise)
training.lda <- lda(f.rating ~., data = training_standardise)
training.lda
# Coefficients of the Discriminant Functions
training.lda$scaling[,1]
# Values of each case for the first discriminant function
training.lda.values <- predict(training.lda, training_standardise[,c(1:6)])
LDA1<- training.lda.values$x[,1]
# LDA2<- training.lda.values$x[,2]
# Separation given by the discriminant functions.
calcSeparations(training.lda.values$x,training_standardise$f.rating)
# Stacked Histograms of the LDA Values
ldahist(data = LDA1, g=training_standardise$f.rating)
# ldahist(data = LDA2, g=training_standardise$f.rating)
plot(LDA1, main="LDA1 Projection of Training Data",
xlab="LDA1")
training_posterior <- training.lda.values$posterior
training_pred <- training.lda.values$class
head(training_posterior)
# Confusion matrix
training_conf <- table(predicted=training_pred, observed=training_standardise$f.rating)
training_conf
# Training Accuracy
training_accuracy<-sum(diag(training_conf))/dim(training_standardise)[1]
training_accuracy
# Training Missclassification Rate
training_mr <- 1-training_accuracy
training_mr
# ggord(training.lda, training_standardise$f.rating)
ggplotLDAPrep <- function(x){
if (!is.null(Terms <- x$terms)) {
data <- model.frame(x)
X <- model.matrix(delete.response(Terms), data)
g <- model.response(data)
xint <- match("(Intercept)", colnames(X), nomatch = 0L)
if (xint > 0L)
X <- X[, -xint, drop = FALSE]
}
means <- colMeans(x$means)
X <- scale(X, center = means, scale = FALSE) %*% x$scaling
rtrn <- as.data.frame(cbind(X,labels=as.character(g)))
rtrn <- data.frame(X,labels=as.character(g))
return(rtrn)
}
fitGraph <- ggplotLDAPrep(training.lda)
ggplot(fitGraph, aes(LD1))+geom_histogram()+facet_wrap(~labels, ncol=1)
ggplot(fitGraph, aes(LD1,fill=labels))+geom_histogram()
# TESTING
test.lda.values <- predict(training.lda, test_standardise[1:6])
test_predictions <- test.lda.values$class
test_posterior <- test.lda.values$posterior
# Confusion matrix
test_conf <- table(predicted=test_predictions, observed=test_standardise$f.rating)
test_conf
# Accuracy
test_accuracy<-sum(diag(test_conf))/dim(test_standardise)[1]
test_accuracy
# Missclassification Rate
test_mr <- 1-test_accuracy
test_mr
names(test.lda.values)
head(test.lda.values$class)
head(test.lda.values$posterior)
head(test.lda.values$x)
par(mfrow=c(1,1))
plot(test.lda.values$x[,1], test.lda.values$class, col=test$f.rating+10)
par(mfrow=c(1,1))
plot(test.lda.values$x[,1], test.lda.values$class, col=test$f.rating)
plot(training.lda, col = as.integer(training$f.rating))
plot(training.lda)
plot(training.lda, col = as.integer(training$f.rating))
load("/Users/ximenamoure/Downloads/student_subset_2015.rda")
load("/Users/ximenamoure/Downloads/student_subset_2015.rda")
prior <- c(alpha =40  , beta =2 )
plot(function(x)dgamma(x, prior[1], prior[2]), xlim=c(0,60), ylab="", xlab = expression(lambda))
title("prior distribution")
x <- rgamma(n=10000, shape=20)
prior <- c(alpha =40  , beta =2 )
plot(function(x)dgamma(x, prior[1], prior[2]), xlim=c(0,60), ylab="", xlab = expression(lambda))
title("prior distribution")
M= 100000
prior.sim <- rgamma(M,prior[1], prior[2])
pre.prior.sim <- rpois(M, prior.sim)
plot(table(pre.prior.sim)/M, col= "blue", lwd= 0.5)
abline(v= c(5,40), lty = 3)
title("prior predictive distribution")
y <- c(21,17,17,19,16,18,15,10,17,16)
n <- length(y)
n
y
sd.like <- function(th) {
(th^sum(y)*exp(-n*th))/integrate(function(th)(th^sum(y)*exp(-n*th)), lower = 0, upper = 50)$value
}
par(mfrow=c(1, 1))
plot(function(x)sd.like(x), xlim=c(0,60), ylab="", xlab = expression(lambda))
plot(function(x)dgamma(x, prior[1], prior[2]), xlim=c(0,60), add=T, lty=2)
legend("topright", c("prior", "likelihood"), lty = c(2,1))
prior <- c(alpha = 1.25 , beta = 25 )
source("~/Downloads/Asthma (1).r")
prior <- c(alpha = 1.25 , beta = 25 )
install.packages(c("ggplot2", "gridExtra"))
library(ggplot2)
library(gridExtra)
install.packages(c("ggplot2", "gridExtra"))
# Given data
y = c(3, 2, 0, 8, 2, 4, 6, 1)
mu_vals = seq(0, 10, 0.01)
bru_prior = dgamma(mu_vals, 0.01, 0.001)
claudia_prior = dgamma(mu_vals, 6.25, 2.5)
carles_prior = rep(1, length(mu_vals))  # flat prior, just a constant value
df_prior = data.frame(mu=mu_vals, Bru=bru_prior, Claudia=claudia_prior, Carles=carles_prior)
p1 = ggplot(df_prior, aes(x=mu)) +
geom_line(aes(y=Bru, color="Bru")) +
geom_line(aes(y=Claudia, color="Cl치udia")) +
geom_line(aes(y=Carles, color="Carles")) +
labs(title="Prior Distributions", y="Density") +
theme(legend.title = element_blank())
library(ggplot2)
p1 = ggplot(df_prior, aes(x=mu)) +
geom_line(aes(y=Bru, color="Bru")) +
geom_line(aes(y=Claudia, color="Cl치udia")) +
geom_line(aes(y=Carles, color="Carles")) +
labs(title="Prior Distributions", y="Density") +
theme(legend.title = element_blank())
bru_posterior = dgamma(mu_vals, 0.01 + sum(y), 0.001 + length(y))
claudia_posterior = dgamma(mu_vals, 6.25 + sum(y), 2.5 + length(y))
carles_posterior = dpois(0:15, lambda=sum(y)/length(y))  # Proportional to likelihood
df_posterior = data.frame(mu=mu_vals, Bru=bru_posterior, Claudia=claudia_posterior, Carles=carles_posterior)
y = c(3, 2, 0, 8, 2, 4, 6, 1)
mu_vals = seq(0, 10, 0.01)
bru_prior = dgamma(mu_vals, 0.01, 0.001)
claudia_prior = dgamma(mu_vals, 6.25, 2.5)
carles_prior = rep(1, length(mu_vals))  # flat prior, just a constant value
df_prior = data.frame(mu=mu_vals, Bru=bru_prior, Claudia=claudia_prior, Carles=carles_prior)
p1 = ggplot(df_prior, aes(x=mu)) +
geom_line(aes(y=Bru, color="Bru")) +
geom_line(aes(y=Claudia, color="Cl치udia")) +
geom_line(aes(y=Carles, color="Carles")) +
labs(title="Prior Distributions", y="Density") +
theme(legend.title = element_blank())
# Likelihood function
likelihood_function <- function(mu, data) {
prod(dpois(data, mu))
}
# Unnormalized posterior for Carles (proportional to likelihood since prior is flat)
carles_unnormalized_posterior = sapply(mu_vals, function(mu) likelihood_function(mu, y))
# Normalize the posterior for Carles
carles_posterior = carles_unnormalized_posterior / sum(carles_unnormalized_posterior * (mu_vals[2] - mu_vals[1]))
# c) Draw the three posterior distributions in the same graph
bru_posterior = dgamma(mu_vals, 0.01 + sum(y), 0.001 + length(y))
claudia_posterior = dgamma(mu_vals, 6.25 + sum(y), 2.5 + length(y))
df_posterior = data.frame(mu=mu_vals, Bru=bru_posterior, Claudia=claudia_posterior, Carles=carles_posterior)
p2 = ggplot(df_posterior, aes(x=mu)) +
geom_line(aes(y=Bru, color="Bru")) +
geom_line(aes(y=Claudia, color="Cl치udia")) +
geom_line(aes(y=Carles, color="Carles")) +
labs(title="Posterior Distributions", y="Density") +
theme(legend.title = element_blank())
# d) Calculate a 90% credible interval for each Bayesian model
# For the gamma distribution, we can use the qgamma function to get quantiles
bru_ci = qgamma(c(0.05, 0.95), 0.01 + sum(y), 0.001 + length(y))
claudia_ci = qgamma(c(0.05, 0.95), 6.25 + sum(y), 2.5 + length(y))
# For Carles' posterior, we'll derive the quantiles from the numeric posterior
carles_ci = quantile(df_posterior$mu, probs=c(0.05, 0.95), weights=df_posterior$Carles)
print(bru_ci)
print(claudia_ci)
print(carles_ci)
# Display the plots
grid.arrange(p1, p2, nrow=2)
library(gridExtra)
grid.arrange(p1, p2, nrow=2)
# dice
plot(1:6, rep(1/6, 6), ty="h", xlab="x", ylab="p(x)", main="dice")
?distribution
#poisson
plot(0:10, dpois(0:10,2), ty="h", xlab="x", ylab="p(x)", main="Poisson(2)")
#binomial
plot(0:10, dbinom(0:10, 10, 0.2), ty="h", xlab="x", ylab="p(x)", main="Binomial(n=10, p=0.2)")
plot(0:50, dbinom(0:50, 50, 0.2), ty="h", xlab="x", ylab="p(x)", main="Binomial(n=10, p=0.2)")
#Normal
plot(function(x)dnorm(x, 0, 1), xlim=c(-5,5), ylab="f(x)")
plot(function(x)dnorm(x, 0, 2), xlim=c(-5,5), lty=2, ylab="f(x)", add=T)
legend("topright", c("Normal(0,1)","Normal(0,2)"), lty=c(1,2))
# probability x~normal(5,3), P(6<x<9)
# exact
pnorm(9,5,3)-pnorm(6,5,3)
# simulation
N <- 1000000
x <- rnorm(N,5,3)
sum(x>6 & x<9)/N
hist(x, breaks=100)
plot(density(x), xlab="x", ylab="f(x)",main="Normal(5,3)")
plot(function(x)dnorm(x, 5,3), xlim=c(-10,10), lty=2, add=T)
legend("topright",c("simulation","exact"), lty=c(1,2))
## Exercise 0.2: COIN AND DICES
n.sim <- 100000
C <- sample(0:1, n.sim, rep=T)
B <- sample(1:6, n.sim, rep=T)
R <- sample(1:6,n.sim,rep=T, prob=c(2/9,1/9,2/9,1/9,2/9,1/9))  # equivalent option
round(table(C)/n.sim,2)
round(table(B)/n.sim,2)
round(table(R)/n.sim,2)
Z <- C*(B+R)
round(table(Z)/n.sim,2)
plot(as.numeric(names(table(Z))),table(Z)/n.sim , type = "h", ylab="p(z)", xlab = "Z", col="blue", lwd=3 ,ylim=c(0,1))
text(as.numeric(names(table(Z))), 0.05 + table(Z)/n.sim, round(table(Z)/n.sim,2))
title("probability distribution of  Z=C*(B+R)")
round(sum(Z>1)/n.sim,3)
knitr::opts_chunk$set(echo = TRUE)
library(genetics)
library(dplyr)
library(HardyWeinberg)
library(data.table)
library(psych)
filename <- "TSIChr22v4.raw"
genetic_data <- fread(filename, drop = c(1:6))
num_variants <- ncol(genetic_data)
missing_data <- sum(is.na(genetic_data))
total_entries <- nrow(genetic_data) * num_variants
missing_data_percentage <- (missing_data / total_entries) * 100
cat("\nNum variants:",num_variants, "\n")
cat("\nPercentage missing:",missing_data_percentage, "\n")
# Identify and remove monomorphic variants
is_monomorphic <- apply(genetic_data, 2, function(x) length(unique(na.omit(x))) == 1)
genetic_data_poly <- genetic_data[, !is_monomorphic, with = FALSE]
# Count remaining variants
remaining_variants <- ncol(genetic_data_poly)
monomorphic_percentage <- (sum(is_monomorphic) / num_variants) * 100
cat("\nPercentage of monomorphic variants:", monomorphic_percentage, "%\n")
cat("\nNumber of remaining variants:", remaining_variants, "\n")
specific_snp <- genetic_data_poly[["rs587756191_T"]]
genotype_counts <- table(factor(specific_snp, levels = 0:2))
# If any genotype is absent, it should be counted as zero
genotype_counts <- c(AA = sum(genotype_counts[1]), AB = sum(genotype_counts[2]), BB = sum(genotype_counts[3]))
# Perform chi-square test, exact test, and permutation test
library(HardyWeinberg)
chi_square_test <- HWChisq(genotype_counts)
exact_test <- HWExact(genotype_counts)
permutation_test <- HWPerm(genotype_counts)
# Print results
print("Chi square test")
print(chi_square_test)
print("#######################")
print("Exact test")
print(exact_test)
print("#######################")
print("Permutation test")
print(permutation_test)
specific_snp <- genetic_data_poly[["rs587756191_T"]]
genotype_counts <- table(factor(specific_snp, levels = 0:2))
# If any genotype is absent, it should be counted as zero
genotype_counts <- c(AA = sum(genotype_counts[1]), AB = sum(genotype_counts[2]), BB = sum(genotype_counts[3]))
# Perform chi-square test, exact test, and permutation test
library(HardyWeinberg)
chi_square_test <- HWChisq(genotype_counts)
exact_test <- HWExact(genotype_counts)
permutation_test <- HWPerm(genotype_counts)
print("Chi square test")
print(chi_square_test)
print("Exact test")
print(exact_test)
print("Permutation test")
print(permutation_test)
specific_snp <- genetic_data_poly[["rs587756191_T"]]
genotype_counts <- table(factor(specific_snp, levels = 0:2))
# If any genotype is absent, it should be counted as zero
genotype_counts <- c(AA = sum(genotype_counts[1]), AB = sum(genotype_counts[2]), BB = sum(genotype_counts[3]))
# Perform chi-square test, exact test, and permutation test
library(HardyWeinberg)
chi_square_test <- HWChisq(genotype_counts)
chi_square_test_without_correction <- HWChisq(genotype_counts, correct = FALSE)
specific_snp <- genetic_data_poly[["rs587756191_T"]]
genotype_counts <- table(factor(specific_snp, levels = 0:2))
# If any genotype is absent, it should be counted as zero
genotype_counts <- c(AA = sum(genotype_counts[1]), AB = sum(genotype_counts[2]), BB = sum(genotype_counts[3]))
# Perform chi-square test, exact test, and permutation test
library(HardyWeinberg)
chi_square_test <- HWChisq(genotype_counts)
chi_square_test_without_correction <- HWChisq(genotype_counts, cc = 0)
exact_test <- HWExact(genotype_counts)
permutation_test <- HWPerm(genotype_counts)
print("Chi square test")
print(chi_square_test)
print("Chi square test without correction")
print(chi_square_test_without_correction)
print("Exact test")
print(exact_test)
print("Permutation test")
print(permutation_test)
#genotype_counts_matrix <- sapply(genetic_data_poly, function(x) table(factor(x, levels = 0:2)))
geno.matrix <- matrix(nrow = 3, ncol = ncol(genetic_data_poly))
geno.matrix <- t(sapply(genetic_data_poly, function(x) {
c(sum(x == 0), sum(x == 1), sum(x == 2))
}))
colnames(geno.matrix) <- c("AA", "AB", "BB")
# Now, each column of geno.matrix contains the counts for AA, AB, and BB for a single SNP.
geno.matrix.exact.pval <- HWExactStats(geno.matrix)
# Determine the number of SNPs with significant deviation from HWE at alpha = 0.05
significant.snp.num.exact <- sum(geno.matrix.exact.pval < 0.05)
# Print the number of significant SNPs
cat("Number of SNPs significantly deviating from HWE:", significant.snp.num.exact, "\n")
# Total number of SNPs tested
total_snps_tested <- nrow(geno.matrix)
percentage_significant <- (significant.snp.num.exact / total_snps_tested) * 100
# Expected number of significant SNPs by chance at alpha = 0.05
expected_significant_by_chance <- total_snps_tested * 0.05
# Print expected number of significant SNPs by chance
cat("Expected number of significant SNPs by chance at alpha = 0.05:", expected_significant_by_chance, "\n")
cat("Percentage of SNPs significantly deviating from HWE at alpha = 0.05:", percentage_significant, "%\n")
# Find the index of the smallest p-value, which indicates the most significant SNP
most_significant_index <- which.min(geno.matrix.exact.pval)
# Retrieve the genotype counts for the most significant SNP
most_significant_counts <- geno.matrix[most_significant_index, ]
# Retrieve the SNP ID for the most significant SNP
# This assumes that the row names of geno.matrix are the SNP IDs
most_significant_snp_id <- rownames(geno.matrix)[most_significant_index]
# Print the ID and genotype counts of the most significant SNP
cat("Most significant SNP:", most_significant_snp_id, "\n")
cat("Genotype counts for the most significant SNP [AA, AB, BB]:", most_significant_counts, "\n")
f.coef <- c()
for(i in 1:ncol(genetic_data_poly)) {
x <- geno.matrix[i,]
names(x) <- c("AA", "AB", "BB")
f.coef <- c(f.coef,HWf(x))
}
hist(f.coef,breaks=10)
f.coef <- c()
for(i in 1:ncol(genetic_data_poly)) {
x <- geno.matrix[i,]
names(x) <- c("AA", "AB", "BB")
f.coef <- c(f.coef,HWf(x))
}
hist(f.coef)
f.coef <- c()
for(i in 1:ncol(genetic_data_poly)) {
x <- geno.matrix[i,]
names(x) <- c("AA", "AB", "BB")
f.coef <- c(f.coef,HWf(x))
}
hist(f.coef, main= "Histogram of Inbreeding Coefficient", xlab = "Inbreeding coefficient")
f.coef.stats <- sapply(f.coef,mean)
summary(f.coef.stats)
sd(f.coef)
plot(density(f.coef))
qqnorm(f.coef, main= "Prob plot of inbreeding coefficient")
qqline(f.coef)
alpha_levels <- c(0.10, 0.05, 0.01, 0.001)
significant_counts <- numeric(length(alpha_levels))
for (i in seq_along(alpha_levels)) {
alpha <- alpha_levels[i]
significant_counts[i] <- sum(geno.matrix.exact.pval < alpha)
}
# Calculate the percentage of significant SNPs for each alpha level
total_snps <- length(geno.matrix.exact.pval)
percentage_significant <- (significant_counts / total_snps) * 100
results_df <- data.frame(
Alpha_Level = alpha_levels,
Significant_SNPs = significant_counts,
Percentage = percentage_significant
)
print(results_df)
setwd("~/Desktop/facultad/SEMESTRE3/BSG/LAB/statistics/BioInf-2")
library(data.table)
library(HardyWeinberg)
dff <- fread("TSIChr22v4.raw",header = T)
df <- dff[,-c(1:6)]
df <- as.data.frame(df)
variants <- ncol(df)
total <- nrow(df) * variants
missing <- sum(is.na(df))
percentage_missing <- missing/total
cat("Number of variants:", variants, "\n")
cat("Percentage of missing data: ", percentage_missing*100, "%\n")
u <- 1
mono <- numeric()
for (i in 1: variants)
{
a <- length(unique(na.omit(df[,i])))
if (a==1){
mono[u] <-  i
u <- u+1
}
}
percentage_mono <- length(mono)/variants;
var_aft <- df[,-mono]
cat("Percentage of monomorphic variants:", percentage_mono*100, "%\n")
cat("Number of remaining variants:", ncol(var_aft), "\n")
rs587756191_T <- var_aft[,"rs587756191_T"]
unique_values <- unique(rs587756191_T)
rs587756191_T <- factor(rs587756191_T, levels = c("0", "1", "2"), labels = c("AA", "AB", "BB"))
tab <- table(rs587756191_T); #Genotype count
gen <- sum(tab)
chi <- HWChisq(tab, cc = 0)
chi_c <- HWChisq(tab, cc = 0.5)
exact <- HWExact(tab)
perm <- HWPerm(tab, nperm = 100)
cat("Genotype Counts for rs587756191_T:\n")
tab
cat("Chi_Square test without continuity:")
print(chi)
cat("Chi_Square test with continuity:")
print(chi_c)
cat("Exact test:")
print(exact)
cat("Permutation test:")
print(perm)
gen_count <- function(genotype_column) {
genotype_column <- factor(genotype_column, levels = c("0", "1", "2"), labels = c("AA", "AB", "BB"))
# Count the occurrences of each genotype
genotype_counts <- table(genotype_column)
return(genotype_counts)
}
# Create datarame
rows <- numeric(ncol(var_aft))
genotypes <- data.frame(AA = rows, AB = rows, BB = rows)
# Calculate gen_count for each variant (column) in var_aft
for (i in seq_along(rows)) {
genotypes[i,] <- as.numeric(gen_count(var_aft[, i]))
}
# Apply HWExactStats test to each row of genotypes
hw_results <- HWExactStats(genotypes,x.linked = FALSE)
significant <- sum(hw_results < 0.05)
percentage_significant <- significant/nrow(genotypes)
cat("Percentage of significant SNPs:",percentage_significant*100 , "%\n")
hw_results$
values <- unlist(hw_results)
ll <- which(hw_results == min(values))
for (i in c(1:nrow(genotypes))) {
x <- genotypes[i,]
x_numeric <- as.numeric(unlist(x))
fhat[i] <- HWf(x_numeric)
}
for (i in c(1:nrow(genotypes))) {
x <- genotypes[i,]
x_numeric <- as.numeric(unlist(x))
fhat[i] <- HWf(x_numeric)
}
significant0.1 <- sum(hw_results < 0.1)
