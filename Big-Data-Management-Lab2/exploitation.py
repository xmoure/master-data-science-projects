from pyspark.sql import SparkSession
import csv

spark = SparkSession \
    .builder \
    .master(f"local[*]") \
    .appName("myApp") \
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
    .getOrCreate()

cultural = spark.read.format("mongo") \
    .option('uri', 'mongodb://localhost:27017/formatted.cultural_equipment') \
    .load() \
    .cache()

idealista = spark.read.format("mongo") \
    .option('uri', 'mongodb://localhost:27017/formatted.idealista') \
    .load() \
    .cache()

income = spark.read.format("mongo") \
    .option('uri', 'mongodb://localhost:27017/formatted.income_opendata') \
    .load() \
    .rdd \
    .cache()

# FIRST KPI: Facilities per neighborhood
culturalRDD = cultural.rdd.map(lambda t: ((t['neighborhood_name_reconciled'], t['district_reconciled'], t['secondary_filters_name']), 1)) \
    .reduceByKey(lambda x, y: x + y) \
    .coalesce(2) \
    .map(lambda t: ((t[0][0], t[0][1]), (t[0][2], t[1]))) \
    .groupByKey() \
    .mapValues(lambda x: dict(x))

formattedRdd = culturalRDD.flatMap(lambda x: [(x[0][0], x[0][1], key, value) for key, value in x[1].items()])
fields = ['neighborhood', 'district', 'culturalType', '#culturalPlaces']
#
transformed_df = formattedRdd.toDF()
transformed_df = transformed_df.withColumnRenamed("_1", "neighborhood") \
    .withColumnRenamed("_2", "district") \
    .withColumnRenamed("_3", "culturalType") \
    .withColumnRenamed("_4", "#culturalPlaces")

transformed_df.write \
    .format("com.mongodb.spark.sql.DefaultSource") \
    .mode("overwrite") \
    .option('uri', f"mongodb://localhost:27017/exploitation.first_kpi") \
    .save()
#
data = formattedRdd.collect()
# Define the output file path
output_file = 'cultural_data.csv'

# Write the data to CSV
with open(output_file, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(fields)
    writer.writerows(data)

# Second KPI: AVG PRICE PER NEIGHBORHOOD PER YEAR

# Map the RDD to key-value pairs
neighborhood_year_price_rdd = idealista.rdd.map(
    lambda x: ((x['neighborhood_name_reconciled'], x['year']), (x['price'], 1)))

# Reduce by key to sum prices and counts
neighborhood_year_sum_count_rdd = neighborhood_year_price_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))

# Calculate average price and include the key in the result tuple
neighborhood_year_avg_price_rdd = neighborhood_year_sum_count_rdd.map(lambda x: (x[0][0], x[0][1], x[1][0] / x[1][1]))

second_to_df = neighborhood_year_avg_price_rdd.toDF()
second_to_df = second_to_df.withColumnRenamed("_1", "neighborhood") \
    .withColumnRenamed("_2", "year") \
    .withColumnRenamed("_3", "avgPrice")
second_to_df.write \
    .format("com.mongodb.spark.sql.DefaultSource") \
    .mode("overwrite") \
    .option('uri', f"mongodb://localhost:27017/exploitation.second_kpi") \
    .save()

# WRITE TO CSV
#  Define the output file path
output_file_second_kpi = 'avg_price.csv'
fields_second_kpi = ["Neighborhood", "Year", "AvgPrice"]
data_second_kpi = neighborhood_year_avg_price_rdd.collect()
d_type = {'AvgPrice': float}
#  Write the data to CSV
with open(output_file_second_kpi, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(fields_second_kpi)
    # Write each row of data
    for row in data_second_kpi:
        neighborhood = row[0]
        year = row[1]
        avg_price = float(row[2])
        writer.writerow([neighborhood, year, avg_price])

#
# # THIRD KPI
kpi3RDD = idealista.rdd.map(lambda t: ((t['neighborhood_name_reconciled'], t['district_reconciled'], t['propertyType']), 1)) \
    .reduceByKey(lambda x, y: x + y) \
    .coalesce(2) \
    .map(lambda t: ((t[0][0], t[0][1]), (t[0][2], t[1]))) \
    .groupByKey() \
    .mapValues(lambda x: dict(x))


flattened_rdd3 = kpi3RDD.flatMap(lambda x: [(x[0][0], x[0][1], key, value) for key, value in x[1].items()])
fields_third_kpi = ['neighborhood', 'district', 'propertyType', '#properties']

rdd3_df = flattened_rdd3.toDF()
rdd3_df = rdd3_df.withColumnRenamed("_1", "neighborhood") \
    .withColumnRenamed("_2", "district") \
    .withColumnRenamed("_3", "propertyType'") \
    .withColumnRenamed("_4", "#properties")

rdd3_df.write \
    .format("com.mongodb.spark.sql.DefaultSource") \
    .mode("overwrite") \
    .option('uri', f"mongodb://localhost:27017/exploitation.third_kpi") \
    .save()

data_kpi3 = flattened_rdd3.collect()
# Define the output file path
output_file_kpi3 = 'kpi3.csv'

# Write the data to CSV
with open(output_file_kpi3, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(fields_third_kpi)
    writer.writerows(data_kpi3)


# Fourth KPI: Average family income for the years 2007 to 2011 compared
# with the average listing price for the years 2020 and 2021.
# This isn't a direct year-to-year comparison, but it can give  an idea of the general differences.
# This is because the years in idealista and in opendata do not coincide
flattened_rdd = income.flatMap(lambda x: [(x['neighborhood_name_reconciled'], info['year'], info['RFD']) for info in x['info']])

idealista_rdd=idealista.rdd
idealista_processed = idealista_rdd.map(lambda x: (x['neighborhood_name_reconciled'], x['year'], x['price']))
avg_listing_prices = idealista_processed.map(lambda x: (x[0], (x[2], 1)))\
                               .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\
                               .mapValues(lambda x: x[0] / x[1])

avg_family_income = flattened_rdd.map(lambda x: (x[0], (x[2], 1)))\
                                     .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\
                                     .mapValues(lambda x: x[0] / x[1])

avg_income_join_av_prices = avg_family_income.join(avg_listing_prices)

results_kpi4 = avg_income_join_av_prices.map(lambda x: (x[0], x[1][0], x[1][1]))

output_file_fourth_kpi = 'fourthKpi.csv'
fields_fourth_kpi = ["Neighborhood", "AvgIncome-2007-2017", "AvgPrice-2020-2021"]
data_fourth_kpi = results_kpi4.collect()

fourth_to_df = results_kpi4.toDF()
rdd4_df = fourth_to_df.withColumnRenamed("_1", "neighborhood") \
    .withColumnRenamed("_2", "AvgIncome-2007-2017") \
    .withColumnRenamed("_3", "AvgPrice-2020-2021'")

rdd4_df.write \
    .format("com.mongodb.spark.sql.DefaultSource") \
    .mode("overwrite") \
    .option('uri', f"mongodb://localhost:27017/exploitation.fourth_kpi") \
    .save()
#  Write the data to CSV
with open(output_file_fourth_kpi, 'w', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(fields_fourth_kpi)
    # Write each row of data
    for row in data_fourth_kpi:
        neighborhood = row[0]
        avg_income = float(row[1])
        avg_price = float(row[2])
        writer.writerow([neighborhood, avg_income, avg_price])
